from flask import Flask, make_response
from pydantic import BaseModel, Field, ValidationError
from langchain_ollama import ChatOllama
from langchain.schema.runnable import RunnableLambda

app = Flask(__name__)

class Answer(BaseModel):
    summary: str = Field(..., max_length=300)
    action: str = Field(..., pattern="^(none|search|email)$")

llm = ChatOllama(model="gemma3", temperature=0)

safe_llm = llm.with_structured_output(Answer)

def reject_html(ans: Answer) -> Answer:
    s = ans.summary.lower()
    for bad in ("<script", "onerror=", "javascript:", "<iframe", "<object", "</"):
        if bad in s:
            raise ValueError("Blocked risky HTML/JS content")
    return ans

chain = safe_llm | RunnableLambda(reject_html)

def html_escape(s: str) -> str:
    return s.replace("&","&amp;").replace("<","&lt;").replace(">","&gt;")

@app.get("/safe")
def safe():
    try:
        ans: Answer = chain.invoke(
            "In two sentences, explain Improper Output Handling (no HTML/JS). "
            "Then choose an action from: none, search, email."
        )
        safe_text = html_escape(ans.summary)
        body = f"<h3>LLM Output (SAFE)</h3><pre>{safe_text}</pre><p>Action: {ans.action}</p>"
        resp = make_response(body)
        resp.headers["Content-Security-Policy"] = (
            "default-src 'self'; script-src 'self'; object-src 'none'; base-uri 'none'"
        )
        return resp
    except (ValidationError, ValueError) as e:
        return f"Blocked: {e}", 400

if __name__ == "__main__":
    app.run(port=5001, debug=True)
